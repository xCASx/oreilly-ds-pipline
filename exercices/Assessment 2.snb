{
  "metadata" : {
    "name" : "Assessment 2",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "${HOME}/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "org.apache.spark %% spark-streaming-kafka-0-8 % _", "com.datastax.spark %% spark-cassandra-connector % 2.0.0-M3", "- org.apache.spark %% spark-core % _", "- org.apache.spark %% spark-streaming % _", "- org.scala-lang % _ % _", "- org.spark-project.spark % _ % _", "- io.netty % _ % _", "- javax.jms % jms % _", "- com.sun.jdmk % jmxtools % _", "- com.sun.jmx % jmxri % _" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.cassandra.connection.host" : "localhost"
    }
  },
  "cells" : [ {
    "metadata" : {
      "id" : "C5C7E9632EAE4DD596F0649E36CA3498"
    },
    "cell_type" : "markdown",
    "source" : "# Assessment 2 (part 1)\n\n## Context\n\nIn this exercise, you will consume kafka with spark Streaming and compute aggregates (average values) for each time window and stock.\n\n## Tasks\n\n* Create a cassandra schema for the mean price aggregate\n* Consume the Kafka \"quotes\" topic\n* Apply transformations to key data by symbol name and time and compute an average\n* Save results to Cassandra"
  }, {
    "metadata" : {
      "id" : "D567253445A9441B93AC56E55F6B0386"
    },
    "cell_type" : "markdown",
    "source" : "### A schema could look like...\n\nWe just want to extract the average price information for a given time window, the schema will be very simple:\n\n```sql\nCREATE TABLE avg_price (\n ...\n \n);\n````\n"
  }, {
    "metadata" : {
      "id" : "57E305D4428C4612809137966D39073E"
    },
    "cell_type" : "markdown",
    "source" : "Usual imports for working with the Cassandra connector"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3CDF13297F98410D8708054EEBB33047"
    },
    "cell_type" : "code",
    "source" : "import com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 756 milliseconds, at 2016-9-20 9:2"
    } ]
  }, {
    "metadata" : {
      "id" : "F60B0A6A9331495C87543CDEF431DFB4"
    },
    "cell_type" : "markdown",
    "source" : "Instanciate the CassandraConnector"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "80CDBCE64EB149848497DC817EAC3F9D"
    },
    "cell_type" : "code",
    "source" : "val cc = CassandraConnector(sparkContext.getConf)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8FA0E01F73A6400585C7BEC243F0C06F"
    },
    "cell_type" : "markdown",
    "source" : "Creation of the Keyspace is needed only if starting the pipeline from scratch."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F929B541F64840418928FD8EBD8DD37B"
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => \n                  session.execute(s\"\"\"\n                    CREATE KEYSPACE IF NOT EXISTS pipeline \n                    WITH REPLICATION = { 'class':'SimpleStrategy', 'replication_factor':1}\n                  \"\"\".stripMargin\n                  )\n                 }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "44F92BC425744DC18D1206739DF99E28"
    },
    "cell_type" : "markdown",
    "source" : "We drop the table if already exists"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "32F1D5149F2F4086AE47BD27049A961F"
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => \n                  session.execute(s\"\"\"\n                    DROP TABLE IF EXISTS pipeline.avg_price\n                  \"\"\"\n                  )}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D2C951472D3E4666836EE5E6ADFC6B67"
    },
    "cell_type" : "markdown",
    "source" : "### Table creation\n\nHere you need to create the table with a simple schema"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C234B3E9984043658850777F837CAA1A"
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => \n                  session.execute(s\"\"\"\n                  CREATE TABLE pipeline.avg_price (\n                    symbol text,\n                    ts timestamp,\n                    price double,\n                    PRIMARY KEY (symbol, ts)\n                  )\n                  \"\"\"\n                  )}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "C197E1B431614F9F8F78E5F5FEB42E70"
    },
    "cell_type" : "markdown",
    "source" : "This Quote case class is used to extract typed data from the quotes stream."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A71E06A13B454DC8B04B213A2DBC3676"
    },
    "cell_type" : "code",
    "source" : "object model extends Serializable {\n  object Quote extends Serializable {\n    val pat = \"\"\"(\\d+),[^,]*,\\\"([^\"]+)\\\",[^,]*,[^,]*,[^,]*,[^,]*,[^,]*,([^,]*),(\\d+),.*\"\"\".r\n    def parse(s: String): Option[Quote] = {\n      try {\n        pat.findFirstMatchIn(s).map(x => Quote(x.subgroups(1), 1000*(x.subgroups(0).toLong/1000),  x.subgroups(2).toDouble, x.subgroups(3).toLong))\n      } catch {\n        case _ : Throwable => None\n      }\n    }\n  } \n  \n  case class Quote(symbol: String, ts: Long, price: Double, volume: Long)\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A4BEA08C4E074E9A858D6954B5ED4520"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "2194620F81AE4DF8BF4B8808D50111F4"
    },
    "cell_type" : "code",
    "source" : "@transient val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "E06E3C4C7767468A8281D694D9AA8441"
    },
    "cell_type" : "markdown",
    "source" : "We define here the StreamingContext and Kafka parameters"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C737266BB4DC4C9ABCF89D094101717B"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\n\nStreamingContext.getActive.foreach(_.stop(false))\n@transient val ssc = new StreamingContext(sc, Seconds(20))\nval brokers = \"localhost:9092\"\nval topics = Set(\"quotes\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CBF34BD2F82D42238D992B484AC324A5"
    },
    "cell_type" : "markdown",
    "source" : "<span style:\"color: red\">Attention:</span> if you want to read the complete data available in the kafka queue, set the ```auto.offset.reset``` property to ```smallest```"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "108F8C10401F46F5B20824BC664E88CB"
    },
    "cell_type" : "code",
    "source" : "//val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n\n// OR START AT SMALLEST OFFSET:\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers, \n                                      \"auto.offset.reset\"    -> \"smallest\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "AE0D3854EB3C41E58A65B5EDD9C9EDDC"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\n// connect (direct) to kafka\n@transient val kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "4D865F1B4C904BF4996B627D611A4692"
    },
    "cell_type" : "markdown",
    "source" : "We transform each RDD directly read from kafka into RDDs of Quote objects"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4C55177A70084AF99A7FBDC2302C0D58"
    },
    "cell_type" : "code",
    "source" : "// transform the CSV Strings into Quotes\n@transient val msgs = kafkaStream.transform { (message: RDD[(String, String)], batchTime: Time) => {\n    // convert each RDD from the batch into a DataFrame\n    message.map(s => model.Quote.parse(s._2)).collect{case Some(q) => q}\n  }\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "698AC77883E54ACA96DDE4B1B1392BF9"
    },
    "cell_type" : "markdown",
    "source" : "#### Core logic for mean values computation:\n\nA possible solution:\n\n* First transform ```RDD[Quote]``` in ```RDD[((String, Long), (Double, Long))]```\n\nThe idea here is to get as key the pair (<i>symbol</i>, <i>timestamp</i>)\n\nWhere <i>timestamp</i> will be the max value from the timestamps in the quotes for this RDD.\n\nYou get this maximum timestamp doing something like:\n\n```scala\nrdd.map(quote => quote.ts).max\n```\n\nThe value is (<i>price</i>, 1). We keep two values because we will reduce using a binary operator and we want in on pass to compute sum and count. Avergae will then be the ratio.\n\n* ```reduceByKey```: for each key elements are combined with a binary operator, the function:\n```scala\n   (x,y) => (x._1 + y._1, x._2 + y._2)\n```\nsums the prices and adds 1's to get the count\n* ```mapValues```is used to compute the ratio between price sum and count to get average\n* finally we flatten the ```((String, Long), Double)``` structure to ```(String, Long, Double)```, we use here pattern matching"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8E9CF2E3E03B489A86142150B72444DD"
    },
    "cell_type" : "code",
    "source" : "@transient val aggs = msgs.transform { rdd => \n                                      val ts =  ???    /// maximum of timestamp in the rdd\n                                      rdd.map{ q => \n                                        ???          /// ((String, Long), (Double, Long)) structure\n                                       }\n                                      }\n                          .reduceByKey{ (x,y) => ??? }   // returns (Double, Long)\n                          .mapValues( ??? )             // (Double, Long) => (Double)\n                          .map{ case ((s,t),v) => (s,t,v)}\n\n  ",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9498E268794C496F8C1EDDC836348D5D"
    },
    "cell_type" : "markdown",
    "source" : "Save to Cassandra here, beware of the schema and column names here the suggestion is:\n```scala\nrdd.toDF(\"symbol\", \"ts\", \"price\")\n```\n\nBut you may have chosen something else in Cassandra"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "52A90BA12DD740219F167EB3825F7E09"
    },
    "cell_type" : "code",
    "source" : "@transient val comp = aggs.foreachRDD {rdd => \n                   rdd.toDF(\"symbol\", \"ts\", \"price\").write.format(\"org.apache.spark.sql.cassandra\")\n                      .mode(org.apache.spark.sql.SaveMode.Append)\n                      .options(Map(\"keyspace\" -> \"pipeline\", \"table\" -> \"avg_price\"))\n                         .save()\n                 }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "567DCA6684F145E88FF8119337475137"
    },
    "cell_type" : "code",
    "source" : "ssc.start()",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}